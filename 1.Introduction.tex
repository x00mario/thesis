\epigraph{The push toward web application capabilities is somewhat frightening once you realize that the boundaries between web applications are very poorly defined, and that nobody is trying to solve that uncomfortable problem first.}%
{\textit{An origin is forever}\\ \textsc{Michal Zalewski}}

%\section{Summarizing this Chapter}
\label{sec:1.introduction}

  The following sections and paragraphs will initially discuss the motivation for creating this thesis and outline the background of the novel defense approach we propose. We will further discuss and categorize formerly published related work, and ultimately shed light on the structure of this work -- cardinally detailing on the impending chapters and sections and their rationale in the scope of this document.

  \section{Motivation and Background}
  \label{subsec:1.2.motivation_and_background}

  Scripting attacks targeted against websites and user agents haven been first documented in 1999 and 2000 by researchers such as Georgi Guninski~\cite{guninski_ie_1999,guninski_ie_1999-1,guninski_iis_2000} and institutions like CERT and the Apache Foundation~\cite{apache_foundation_cross_2000,carnegie_mellon_university_cert_2000}. The first reported real-life attack vectors targeted early web applications, attempting to break their security model, and web browsers, aiming for code execution vectors and cross-context script execution. J. Topf published a different attack vector in 2001, describing how HTML forms can be used to unidirectionally communicate with non-HTTP services from within the browser by using \textit{textarea} elements and specially created line-separated messages sent across domain and port borders; this was used to attack IRC, POP3, SMTP and other protocols~\cite{topf2001html}.\\

  These vectors defined a new set of attack techniques and vulnerability classes because they relied on novel features installed in user agents such as browsers, news readers, instant messaging (IM) clients, as well as email clients. With the rise of features and technologies like frames and scripting for websites and comparable documents, vendors have created a new layer of data processing and presentation in their software and thus delivered the foundation for these kinds of attacks. According to D. Ross, the attack was christened Cross Site Scripting by Microsoft engineers in January 2000~\footnote{Ross, D., \textit{Happy 10th birthday Cross-Site Scripting!}, \url{http://blogs.msdn.com/b/dross/archive/2009/12/15/happy-10th-birthday-cross-site-scripting.aspx} (Dec 2009)}.\\

  More than ten years have passed but aside from slight technical deviations the ways of attacking websites and user agents have not significantly changed since. Meanwhile, in strong contrast to the late nineties and the early 2000s, scripting attack techniques have gained considerable attention of a far broader audience. Upon the turn of the millennium, early years brought attack vectors into exclusive discussions on dedicated mailing lists such as Bugtraq, and later, in 2002, by the list Full Disclosure\cite{bugtraq_bugtraq:_1999,full_disclosure_full_2002}. It was known to be a rather common behavior of software vendors to discount these attacks with very little attention, leaving critical software bugs unfixed for months if not years\cite{reavis_csoinformer_2000}.\\

  Nowadays -- similarly to the late 1990s -- a large percentage of web applications still suffers from XSS bugs and exploits: Those are caused by improperly filtered content originating from sources such as a vulnerable user agent, a different web application or simple the web application user. Further reasons for less obvious XSS vulnerabilities are being discussed in detail in Section~\ref{sec:5.attacking_existing_mitigation_approaches}. Several sources state that an estimate of 30\% of the analyzed websites contain XSS vulnerabilities, among those sources one can find the 2007 Symantec Internet Security Threat Report~\cite{turner_symantec_2008}. At the same time browser exploits based on scripting vectors are usually caused either by insufficient filtering of user input or data transmitted via HTTP and similar browser supported protocols attempting to cross borders between scripting contexts or execute browser functions with maliciously prepared parameters -- our research indicates that this number could even be an underestimation.\\

  Over the course of recent years, browsers and web applications have grown in terms of complexity, providing more functionality and interfaces for user interaction, data persistence and improved rendering and layout. User agents have been given numerous interfaces for direct communication with the underlying operating systems and neighbored applications -- including for instance ActiveX, file down- and uploads, Web GL, printing and device communication. Still, the majority of sanitation and mitigation techniques relies on simple string analysis, comparison, modification and pattern recognition. Even highly sophisticated filtering and sanitation software, such as the HTMLPurifier, a tool discussed in Section~\ref{subsubsec:4.3.5.rewriting_code}, in the end compares string fragments against white-list entries. In doing so, it attempts to determine security risks of those strings depending on configuration, all without being able to take the surrounding context into consideration.\\

  The motivation behind this thesis is to dissect and comprehend the anatomy of scripting-based web and browser attacks, outline and review mitigation and defense mechanisms incorporated over the last decade, and identify new forms of vulnerabilities and attack vectors capable of existing mitigation approaches' bypassing. Ultimately, the thesis will in detail portray the complexity of these mitigation tasks, pinpoint the mistakes that have been made over the past ten years, and attempt to elucidate a novel approach to encountering script-based web and browser exploits and attack techniques. A key contribution of this dissertation includes a description and discussion of a foundation capable of burgeoning browser and web security towards a safer web without rewriting its core. While a clean slate approach might promise more effective security, privacy and trust enforcement mechanisms, its feasibility can be considered rather improbable given the sheer mass of existing web documents, the complexity of the existing infrastructure and the dependencies to other technologies.\\
  
  
  \section{Related Work}
  \label{subsec:1.3.related_work}

  The recent years have brought us a significantly large body of research into scripting-based web attacks and browser exploits. Similarly, language formalization, security-driven language isolation and runtime enforcements based on current and future JavaScript implementations as well as static code analysis checking existing code for suspicious patterns based on a set of policies, have been made subjects of extensive research coverage. The relevant research fields covered by this thesis can be roughly split into three major parts: 

  \begin{itemize}
    %
    \item \textbf{Scripting attacks against websites and web applications}; This research is of significant relevance for this thesis since it describes and discusses the numerous ways of attacks against web applications and online documents necessary to comprehend before attempting to design a holistic DOM based protection approach. Attackers fall back to a tremendously large base of attacks and vectors to accomplish their malicious goals -- research covering those activities and techniques therefore is an invaluable contribution for a novel protection tool.
    %
    \item \textbf{Formalization, isolation and runtime enforcements in scripting languages}; This reserach is especially relevant for the context of this thesis, since this research contributes to a deeper understanding of loosely typed and syntactically flexible scripting languages and interface access to sensitive properties in the browser. Understanding the pitfalls of modern JavaScript parsers and DOM implementations is substantial for being able to create a purely DOM based protection library without exposing it to attacks and causing additional vulnerabilities as well as data leakage sources.
    %
    \item \textbf{Attacks against web browsers utilizing scripting techniques}; While scripting attacks against websites usually target information retrieval related goals to obtain login credentials, bank account data and other sensitive data, scripting attacks against browsers have different goals -- including code execution and operation system level compromise. The research on this field is relevant for the scope of our thesis, since those vulnerabilities can be leveraged by website injection flaws and should be in scope for a holistic DOM-based protection software. 
    %
  \end{itemize}

   The latter of the three fields is heavily present in published work due to its  connection to online Phishing attacks -- and defense techniques attempting to add stronger authentication features to websites requiring sensitive user data. As it remains out of scope for this thesis, Phishing will only be covered marginally. Nevertheless, drive-by Pharming and other attacks simply requesting sensitive resources via HTTP and comparable will be included, as the technical aspect of scripting attacks against browser components and underlying layers, drive-by downloads and remote code execution (RCE) via JavaScript inclusive, hold their relevance.

  \begin{itemize}
    %
    \item \textbf{Scripting attacks against websites and web applications} 
      Cross Site Scripting attacks against web applications are featured in this first section of literature review. They have been covered by a great variety of research, especially in 2008 and 2009. Additionally, the year 2006 has generated two important publications which dealt with injection attacks against web applications and proposed unorthodox ways of analyzing incoming data and checking for vulnerabilities as well as protecting against ongoing attacks. Earlier research also cannot be discredited and will be mentioned. To begin, one has to refer to Pietraszek et al., who introduced CSSE; this is a library to examine strings of incoming user-generated data by relying on a set of meta-data~\cite{pietraszek2006defending}. Depending on the context derived from the attached meta-data, different filtering and escaping methods were being applied for the protection of the existing applications. This low-level approach is described as applicable for existing applications, requiring few to no application developer implementation effort. Comparable work has been put forward even earlier on by Ismail et al.; in 2004, these authors have lain foundations for a detection and collection tool residing on the client and preventing XSS attacks, as well as sending out warnings to application owners in case an attack was detected~\cite{ismail2004proposal}. Vogt et al. suggested data tainting as possible cure against reflected and DOM based XSS attacks in 2007~\cite{vogt2007cross}. This approach included modification of the Mozilla Firefox browsers to be able to provide the necessary tainting features in a website DOM. \\

      Second of the above-mentioned key publications in 2006  was work of Kirda et al.~\cite{kirda2006noxes} on web application attack mitigation labeled NoXSS, which according to their publication was a primer in client-side XSS defense~\cite{kirda2006noxes}. The authors describe the difficulties of server-side XSS detection and prevention based on the manifold of encoding and obfuscation techniques an attacker can choose from. Simultaneously, they propose a client-side web proxy attached between operating system and web browser to intercept and analyze web traffic before being processed and rendered by the user agent. A noteworthy feature of NoXSS is the snapshot mode allowing a user to train the web proxy on frequented websites before switching it into a defense mode; thus it can detect scripting anomalies and indicate and oppress possible attacks. A more offense-driven approach in researching XSS attacks has been presented in 2008 by Martin et al.; their tool called QED is meant to be used for analyzing Java web applications following the servlet code specifications and claims, consequently producing comprehensive results yielding no false alerts: It uses a goal-directed model checking system only reporting vulnerabilities in case the system could create a successful exploit~\cite{martin2008automatic}. \\

      Progressing on a time axis, Wassermann et al. recounted XSS attack detection and prevention based on static code analysis in 2008~\cite{wassermann2008static}. Their work covered the problem of obfuscated markup and invalid HTML tag- and attribute-syntax for bypassing the existing XSS filters. The researchers explicitly mentioned the possibilities attackers possess to abuse the permissive parsing user agents perform for sneaking past IDS detection rules and server-side HTML sanitizers. DOMXSS-based attacks have not been taken into account by their defense solution though. Later in 2008 Johns, Engelmann and Posegga introduced XSSDS (Static Detection of Cross-Site Scripting Vulnerabilities); this is a passive and server-side XSS detection system trained with an overall of 500.000 recorded HTTP requests~\cite{johns2008xssds}. XSSDS compares HTTP request URI and resulting markup -- searches matches and consequently judges probability for an attack attempt. Note that nevertheless either stored XSS and DOMXSS are either hard to detect or simply out of scope for XSSDS -- we will discuss attempts to cover mitigation of those in Chapter~\ref{ch:4:novel-defense-approaches}. \\

      Following this train of thought, Kieyzun et al. introduced their developments into the field in 2009. Their tool is capable of automatically generating XSS and SQL injection attack strings against web applications~\cite{kieyzun2009automatic}. Similarly to XSSDS, an output matching takes place to qualify the attack probability here as well. Their framework Blueprint utilizes two components to sanitize and render untrusted content. A server-side application encodes this content into a model representation that can be processed by the client-side part of the tool (padded Base64 is being used; several attacks based on browser bugs presented in Chapter~\ref{ch:3:mitigation-and-bypass} will bypass the Blueprint-provided protection). Upon successful reception of this data representation, the client-side component can decode the content based on high-level policies. Effectually, it can make use of the browser features to build a safe DOM representation, thus avoiding filtering pitfalls server-side libraries are often prone to. Nadji et al. proposed a similar approach. They use a server-client-based system to engage XSS attacks by enforcing the respective document structure integrity (DSI)~\cite{nadji2009document}. Similar to the inner workings of the HTMLPurifier, the untrusted input here has to follow rules defined by integrity policies before being rendered by the user agent. The authors made a strong claim about server-side defense against XSS as a standalone approach being powerless against the existing and documented pool of attack techniques. The research published by Barth et al. gave an account of content sniffing problems causing data leakage on modern web browsers and the common problem of cross-origin leakage of JavaScript and DOM properties abetting data leakage and XSS vulnerabilities~\cite{barth2009cross,barth2009secure}. In the same year, Wurzinger et al. introduced SWAP. This was yet another novel approach to addressing XSS attacks and vulnerabilities through an installation of a reverse proxy, which is using an instrumented user agent to find out if JavaScript execution happened. It then reacted accordingly while being aware of the limitations regarding overhead and impedance mismatches, as well as specific user agents peculiarities~\cite{wurzinger2009swap}.\\

      In 2010, Saxena, Molnar and Livshits presented ScriptGuard -- a context-sensitive XSS sanitation tool capable of automatic context detection and accordant sanitation routine selection~\cite{saxena2010scriptgard}. Later that year, Saxena et al. published on client-side validation vulnerabilities (CSV), this time focusing on attacks invisible or incomprehensible for server=side injection filters and sanitizers. They acquainted the community with FLAX, a toolkit designed to spot client-side validation vulnerabilities by fuzzing and data tainting. Bates et al. also recalled severe security vulnerabilities in client-side XSS filters, underlining the risk potential of bluntly matching request URI to request body. They equally pinpointed possibilities to utilize client-side-only XSS detection mechanisms to leverage XSS attacks despite the presence of well protected web applications\cite{bates2010regular}. \\

      Ultimately, in 2011, Weinberger et al. have published a technical report on empirical analysis of XSS sanitation in web application frameworks~\cite{weinberger2011empirical}. Their evaluation results clearly indicate a severe lack of sufficient input filtering mechanisms in most modern web application frameworks, a phenomenon which is in turn leading to hard to avoid sources and sinks for XSS attacks in live web applications. In the following publication Weinberger et al. voice their opinion that server-side frameworks are cursed with visibility problems for several subsets of XSS attacks and thus cannot sufficiently fulfill their protective duties~\cite{weinberger2011systematic}. While most of the frameworks are capable of filtering and sanitizing HTML properly, a vast majority of them still lack context sensitivity and filter techniques sufficient for user controlled JavaScript, JSON or CSS. \\
      
      Heiderich et al. have also recently published on XSS vulnerabilities caused by SVG graphics bypassing modern HTML sanitizers as well DOM based attack detection in the context of browser malware and complex cross context scripting attacks~\cite{heiderich2011iceshield,heiderich2011crouchingtiger}. We will elaborate in more detail on those attack technqiues and their security implications in Section~\ref{subsubsec:5.4.8.attacks_using_innerhtml} \\
      
    %
    \item \textbf{Formalization, isolation and runtime enforcements in scripting languages} 
      
      In this second subfield of relevant literature mostly recent sources have to be referred to. The issues in question have been extensively investigated by Maffeis et al. who discussed the possibilities that languages like JavaScript provide for creation of safe and isolated runtime environments. The main goal of this research has been to determine capabilities of existing language specifications to deliver -- and by enumerating existing limitations help upcoming specifications to improve and provide the necessary foundations~\cite{MMT-APLAS-TR08,MMT-CSF-TR09,mmt-esorics09}. \\ 

      In 2007, Yu et al. proposed to utilize JavaScript code rewriting to thwart security risks and mitigate both XSS and related attack patterns~\cite{yu2007javascript}. Their approach called CoreScript attempts to reach grander applicability by allowing higher order script and pre-definitions of subset of JavaScript considered to be safe for execution. They proposed a rewriting wrapping alongside channeling all script snippets on a website to a central policy enforcer in attempt to sustain a consistent level of script security. Yu et al. base their research on Anderson et al. and Thiemann from 2005 ~\cite{anderson2005towards,thiemann2005towards}. \\

      In 2008 Maffeis et al. published on operational semantics for JavaScript~\cite{MMT-APLAS-TR08}, examining JavaScript 1.5/ECMA Script 3 for its feasibility for security critical use cases in web application mash-ups and similar installations. Those were, as stated in the publication, later used as a basis for security analysis and implementation in libraries such as Yahoo! ADSafe, Facebook JavaScript (FBJS) and Google Caja. \\

      In 2009, a follow-up publication by Maffeis et al. covered the language-based isolation of untrusted JavaScript~\cite{MMT-CSF-TR09} -- analyzing real life use cases of untrusted yet presumably isolated JavaScript based mash-up applications. The outcome of this work was the  spotting of several design-based security vulnerabilities in FBJS, as well as implementing working fix based on the operation semantics proposed in~\cite{MMT-APLAS-TR08}. Later in 2009, Maffeis, Mitchell and Taly circulated their work on runtime enforcements of secure JavaScript subsets focusing on ECMA-Script 262 compliant JavaScript and properties hindering effective isolation: Again, the FBJS sand-boxing is used as a practical example supporting the research and underlining the necessity for more thorough isolation in design and implementation. Maffeis, Mitchell and Taly have subsequently published on their ongoing research into JavaScript isolation and runtime enforcements. The latter paper encompassed rewriting and wrapping approaches as implemented in libraries such as Google Caja~\cite{mmt-esorics09}. Google Caja and the attempt to enable safe active content by sanitizing and rewriting JavaScript has been also described and introduced into the debates by Miller et al. in 2008~\cite{miller2008caja}. \\
  
      Guarnieri and Livshits conducted their research into Gatekeeper; this is a static analysis tool, which is able to enforce strict security policies in JavaScript code ~\cite{guarnieri2009gatekeeper}. Contrary to the formalization-based isolation approaches proposed by Maffeis et al., Gatekeeper is designed as a static code analysis tool acting as a gateway between untrusted widget code and the website the widget is supposed to be deployed on. Note that Gatekeeper seeks to identify possibly malicious code snippets and block deployment -- rather than limit object capabilities or rewrite existing untrusted code to represent a safe JavaScript subset. \\

      Unlike Gatekeeper study results, the conclusions from research conducted by Chugh et al. published in 2009 did not rely on static code analysis to qualify untrusted JavaScript but instead proposed information flow analysis~\cite{chugh2009staged}. Chugh et al. have defined policies for variable access bound to the code flow; this makes it capable of handling higher order script and dynamically generated code. This approach is related to the work comprising this thesis and follows in the footsteps of research by Hallaraker and Vigna submitted in 2005~\cite{hallaraker2005detecting}: Those authors proposed an approach capable of monitoring, qualifying and limiting JavaScript code at runtime. Executed code is being compared to high-level policies -- sensitive properties can only be accessed if the policy requirements are met. Hallaraker and Vigna offered a client-side IDS embedded directly in the user agent. Using the Mozilla browser as an example, they however concluded by admitting that their proposal is flawed by its complexity and browser design based trade offs. \\

      In 2010, Maffeis et al. published their research on object capabilities and isolation of untrusted web applications -- again covering the capability safe JavaScript and HTML rewriting library Google Caja. They deliver a proof for safety of an authority-safe security model despite the lack of existence of an underlying object capability model~\cite{mmt-oakland10}. This research was dedicated to deliver formal proof of the security models implemented and enforced by the Google Caja code rewriting system Cajita. Cutsem and Miller have written on language proxies in 2010, tying up to Maffeis' et al. research in 2009, they were using JavaScript as the language of choice for their practical examples thanks to a prototype of the Tracemonkey JavaScript engine created by Andreas Gal for this very purpose. Their work focuses on JavaScript extensions enabling usage of proxies, wrappers and reflection capabilities and it was later successfully used as a foundation for creating the ECMA-Script 5 / ECMA-Script 263 specification drafts. Due to the immane importance, several of their suggestions and research results are utilized for the proposal in Chapter~\ref{ch:4:novel-defense-approaches} of this thesis. \\

     Last but not least, Phung et al. published on self-protecting lightweight JavaScript, proposing a self-monitoring JavaScript meta-programming layer based on proprietary JavaScript features~\cite{phung_lightweight_2009}. Their approach propounded interception and consequent reflection for getter and setter access in DOM environments -- backed by JavaScript policy files. Their approach uses techniques similar to aspect-oriented programming techniques in JavaScript and addresses malware and XSS attacks, yet it relies on non-standard features in its prototypic implementation. To sum up, their research results provided noisy but operational interception support, thus marking an important step in thriving towards XSS and malware detection with native JavaScript. A publication evaluating robustness and tamper safety of the aforementioned wrapping technique has been released by Magazinius et al., who have discussed real-life attacks and bypasses as well as mitigation attempts~\cite{magazinius_safe_2010}. \\

    %
    \item \textbf{Attacks against web browsers using scripting techniques} 

      In 2005 and early 2006 several publications were compiled and released in order to inform about web-based malware attacking browsers to deploy their payload. Most of the papers focused on empiric studies with determinations of percentage of websites that could have been considered malicious. Milletary et al. had delivered a technical report on technical foundations for Phishing attacks and browser malware delivery and essentially recommended a raised level of awareness and usage of defensive and informative browser tool-bars as mitigation best practice~\cite{milletary2005technical}. Moshchuk et al. presented a crawler-based study on web-based malware in 2006 - showing 13.2\% of all crawled executable files to be of malicious intent. Furthermore, an alarming number of 5.9\% were described as malicious in terms of containing drive-by-download code~\cite{moshchuk2006crawler}. Wang et al. presented Strider HoneyMonkeys in the same year. Their approach relied on a crawling-based patrol system working in a cost-optimized manner to analyze websites and spot potential occasions for web-based malware -- using an array of virtual machines operating on various vulnerable patch levels. \\

      In 2007, Provos et al. discussed the most prevalent mechanisms used by drive-by-download attacks~\cite{provos2007ghost}. Their work identified problems regarding web server security, user generated content, third party widgets and, finally, advertising scripts as main sources for browser malware deployment. It also elaborated in detail on exploitation techniques using JavaScript and comparable scripting languages. Johns presented research describing web malware and XSS attacks with the combination of website targeted attacks and drive-by-download malware and code attempting to cross contextual borders and access resources on the victim's file system~\cite{johns2008javascript}. The importance of his research is underlinded for explicitly mentioning attacks against Intranets, browser cache and timing vulnerabilities. \\
 
      Provos et al. circulated their results of examining a massive amount of URLs possibly pointing to malicious code  and spotting over three million malware infected resources among them~\cite{provos2008all}. Additionally, they have reported 1.3\% of URLs returned for user generated Google search queries as containing either malicious software or at least risks for the users visiting them. Roughly same time, Louw et al. discussed vulnerabilities and the general lack of security concepts in modern browser's extensions and plug-ins, showing that attackers not only use browsers but especially the often less secure plug-ins and extensions to carry out their payload~\cite{ter2008enhancing}. This work was later followed up by Barth's et al. research on a practical security model for browser extensions ~\cite{barth2010protecting}. In 2009, Egele et al. deliberated about possible mitigations of web malware attacking browser vulnerabilities, especially by talking over the techniques to handle heap-spray attacks and similar ways to prepare and exploit crashes and code execution vulnerabilities in modern browsers and their plug-ins~\cite{egele2009defending}. Reis et al. describe three factors as the cornerstones of browser security, in their opinion including: severity of vulnerabilities, window of vulnerability and frequency of exposure~\cite{reis2009browser}. Wang et al. introduced Gazelle in 2009, referencing Reis' work on the proposed Google Chrome architecture as they outlined their thoughts on a secure browser built. \\

      In 2010, Cova et al. published their research on JavaScript malware and obfuscated attack code. Their work resulted in the library JSAND and the tool Wepawet~\cite{cova2010detection}. Cova and colleagues focused on detection and classification of web malware, while at the same time building a web malware database to detect patterns and support later research on the topic. Their approach is using machine learning to train an anomaly detection algorithms. The resulting software is capable of telling apart benign and malicious JavaScript and similar client-side code based on the detected anomaly level. Likewise, Rieck et al. presented Cujo; this is a system for automatic detection and prevention of the delivery of malicious JavaScript code~\cite{rieck2010cujo}. Cujo, unlike Wepawet being designed as high-interaction honey-client, acts as a transparent web proxy, promising faster detection rates and raised practicability for live-analysis and protection of usual web traffic.

      Curtsinger et al. have moved their research to one layer above and directly embedded their proposal in the user agents: Zozzle is based on their former research on Nozzle~\cite{ratanaworabhan2009nozzle} and hooks into browser functionality to analyze processed code and judge it with static code analysis~\cite{curtsinger2011zozzle}. Their approach of hooking into the \textit{eval} statement, has promised better results than plain static code analysis. The reason behind it is that several obfuscation layers attackers commonly use have been decoded already. In the same year, Carnali et al. introduced Prophiler, which is a tool designed to detect and filter malicious content in websites -- specialized on better performance than usual dynamic analysis tools without significant break-down in detection rate or false alert avoidance~\cite{canali2011prophiler}. Prophiler utilizes a similar set of features for detection and classification as Wepawet but adds a significant amount of new detection rules to operate as a web filter rather than pose a standalone high-interaction honey-client solution.
    %
  \end{itemize}

  The three above-delineated fields of research lay the foundation for research-input and general outcome of this thesis, as well as later projects expanded even further and targeted towards an effective and practical mitigation and elimination of the client-side scripting attacks' impact. In essence, they are providing a basic framework for a development of novel client-secure web applications and the belated security implementations for existing applications and mash-ups. \\


  \section{Contribution and Outlook}
  \label{subsec:1.4.contribution_and_outlook}

  The main contribution of this thesis lies in definition, explanation and discussion of the JavaScript-based DOM framework capable of eradicating the root consequences of Cross Site Scripting attacks. It is crucial that the framework in question is working on the majority of modern browsers, including Internet Explorer, Gecko-based user agents, Safari, Google Chrome and -- to some extent-- Opera browser. The rationale for such a DOM-based approach is being delivered via empiric study of existing XSS- and scripting-based web attack mitigation techniques, proving them weak or even useless, varying in regards to the context and attack vector. In Chapter~\ref{ch:3:mitigation-and-bypass}, several real-life attacks with bypassing of state of the art filtering libraries, mitigation approaches and defense techniques designed to protect web applications from scripting attacks will be demonstrated. Furthermore, this chapter will illustrate and thrash out the common approach of addressing XSS and scripting-based attacks with server-side solutions. It will additionally underline the necessity for a client-based approach justified by attack complexity, visibility and impedance mismatches caused by loss of information between server- and client-side transport and presentation layers.\\

  In Chapter~\ref{ch:4:novel-defense-approaches}, a prototypic library based on ECMA Script 5 object extensions is being presented. Its capabilities of protecting important DOM assets, monitoring property access and function calls, building the foundation for a DOM based IDS/IPS are being highlighted. This framework provides a flexible yet performative fundament for extending the protection range and cover attack vectors such as Clickjacking~\cite{balduzzi2010solution,rydstedt_busting_2010} and UI redressing attacks~\cite{niemietz2011uiRedressing}. \\

  The here-proposed framework has an indisputable advantage of being easily implemented by application developers -- without rewriting existing code -- as it is consisting of a single JavaScript and an optional policy definition source. This leads to a broader discussion of existing frameworks for DOM based attack mitigation and defense such as CSSReg, JSReg~\cite{heyes_jsreg_2011} and similar libraries examined for their potential contributions to the aforementioned framework, namely increasing its protection level. The current limitations and an outlook based on upcoming browser security features and ECMA script specification progress will be addressed as well -- focusing on DOM proxies, membranes and possibilities for easy DOM-based access control/Role based Access Control (RBAC) systems and solutions to defend attacks against the framework based on illegitimate DOM location property settings, discussed in Section~\ref{subsubsec:6.7.2.taming_javascript_uris}. RBAC has been discussed by Ferraiolo and thus far found application in several security critical implementations such as operating systems~\cite{ferraiolo1995role}. We believe that RBAC policy enforcements will drastically increase the security level available for web applications and online documents. The Mozilla Foundation proposes RBAC for usage in BrowserID -- a project implementing a shared ID for users among various computers and browser installations~\footnote{Destuynder, G., \textit{BrowserID: System security}, \url{https://blog.mozilla.com/webappsec/2012/02/03/browserid-system-security/} (Feb 2012)}.\\

  The final part of the thesis challenges and refers to the future possibilities for framework usage and extensions. Conclusion will bring  an insight combining latest browser trends, ECMA script specification drafts and theoretical attack vectors resulting from changes in HTML5 and its
 specification subsets. Final goal of this work is to provide a major step towards eradication of XSS based on awareness and policy driven client-side web application protection and rule enforcement. Essentially, a proof that few steps are necessary to a provision of a fully working solution covering a large percentage of user agents -- while reducing complexity and easing implementation for web application developers is supplied. \\

  \section{Thesis Outline}
  \label{subsec:1.1.thesis_overview}

  This thesis is divided into four major parts. Part one, ``Introduction''~\ref{ch:1:introduction}, provides an outline for the topics discussed in the later chapters. The motivation behind this study is then being discussed and followed by Section~\ref{subsec:1.3.related_work}, which highlights related work in the field of this research.  Most importantly, first chapter introduces a dilemma that motivated research on scripting web and browser attacks leading to the making of this thesis. Furthermore, Section~\ref{subsec:1.4.contribution_and_outlook} talks of a scientific contribution of this work and supply a short suggestion-section on future work, whilst justifying the choice of the title of the document as containing the phrase ``Thriving towards''.\\

  The second chapter ``Browser and Web Security''~\ref{ch:2:browser-and-web-security} is dedicated to a broad and thorough overview of the current state of security challenges and defensive responses in the field of browser security. The sections compiling this chapter expand upon on browser security mechanisms, possible bypasses and a split between rich features fulfilling developer requirements, specification demand and end user convenience, as it pertains to browser vendors and their implementation work. Following the sections on browser security, the foundations of modern web security are introduced and discussed in order to shed light on conflicted areas between usability and security, as well as provide an outlook on features and defense techniques expected during the upcoming months and years.\\

  The third chapter called ``Mitigation and Bypass''~\ref{ch:3:mitigation-and-bypass} focuses on how the aforementioned browser and web security aspects are being addressed by defensive techniques and mitigation approaches. Each of the here-introduced techniques, best practices and products are put under tight scrutiny to determine its defensive value, possibilities of being bypassed or completely subverted in a security sense, and benchmarked with several documented and so far undocumented attack techniques. Ultimately, this chapter concludes with a section amassing the results of security evaluations and laying the foundation and rationale for Chapter~\ref{ch:4:novel-defense-approaches}.\\

  The fourth chapter entitled~``Novel Defense Approaches''~\ref{ch:4:novel-defense-approaches} introduces both academic and non-academic research into novel defense techniques. Those are aspiring to mitigate and eliminate risks and consequences of client-side scripting attacks against websites and browsers. An introductory part will constitute a commentary on important DOM properties and methods available in modern user agents, outline the progress and status quo of DOM-based meta-programming techniques and eventually show how current and future Document Object Model (DOM) implementations can be utilized to create a novel intrusion detection and prevention layer capable of mitigating the bypass techniques discussed in chapter~\ref{ch:3:mitigation-and-bypass}. This chapter additionally contains a technical discussion and source code examples of an existing prototype used to detect and mitigate real-life DOM based browser exploits, applying the results of this work to a prototypic working DOM-based XSS protection tool -- capable of being deployed on a variety of modern websites and user agents without a noticeable effects on usability and performance. The chapter concludes with a section on remaining limitations, existing pitfalls, and potential future work.\\

  Chapter five~\ref{ch:7:outlook-and-future-work} outlines future work considerations on design and implementation of our DOM based protection approach, describes plans for policy generation and other aspects of clean and robust implementation works and concludes with an outlook and impact predictions for a working and deploy-ready gamma release (contrary to the current alpha state of our proposed DOM protection and malware defense libraries). \\

  The sixth and final part of the thesis consists of a list of tables' frame, figures and listings, acknowledgements and curriculum vitae of the author.\\

  